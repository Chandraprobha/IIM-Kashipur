# -*- coding: utf-8 -*-
"""NLP Green Finance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-CC3EAlbbg2wZrP5TaAq6vRbA-DJ4WRb
"""

!pip install pdfplumber

import pdfplumber

!pip install spacy nltk pdfplumber transformers
!python -m spacy download en_core_web_sm
!pip install vaderSentiment

import spacy
from nltk.sentiment.vader import SentimentIntensityAnalyzer

!pip install transformers==4.31.0

import pdfplumber
import spacy
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# Step 1: Extract text from PDF
pdf_file_path = "/content/2021 01 - RBI Bulletin Article - Green Finance in India - Progress and Challenges.pdf"

article_text = ""
with pdfplumber.open(pdf_file_path) as pdf:
    for page in pdf.pages:
        article_text += page.extract_text() + "\n"

# Step 2: Load SpaCy NLP model
nlp = spacy.load("en_core_web_sm")

# Step 3: Process the extracted text with SpaCy
doc = nlp(article_text)

# POS Tagging
pos_tags = [(token.text, token.pos_) for token in doc]

# Named Entity Recognition (NER)
entities = [(entity.text, entity.label_) for entity in doc.ents]

# Step 4: Perform Sentiment Analysis
nltk.download('vader_lexicon')  # Download the VADER lexicon if not already available
sia = SentimentIntensityAnalyzer()
sentiment = sia.polarity_scores(article_text)

# Step 5: Summarization using Hugging Face Transformers
model_name = "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)

# Function to chunk text and summarize
def summarize_large_text(text, max_chunk_size=1024):
    # Split the text into manageable chunks
    chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]
    summaries = []

    for chunk in chunks:
        # Summarize each chunk
        summary = summarizer(chunk, max_length=130, min_length=30, do_sample=False)
        summaries.append(summary[0]['summary_text'])

    return "\n".join(summaries)

# Generate summary for the article
summary = summarize_large_text(article_text)

# Print results
print("Extracted Text:\n", article_text)
print("\nPart-of-Speech Tags:", pos_tags)
print("\nNamed Entities:", entities)
print("\nSentiment Scores:", sentiment)
print("\nSummary:", summary)



